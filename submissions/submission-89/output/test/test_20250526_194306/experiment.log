2025-05-26 19:43:06 - training.experiment - INFO - Experiment 'quick_test' initialized with ID: test_20250526_194306
2025-05-26 19:43:06 - training.experiment - INFO - Created data loaders with batch sizes: train=16, val=64
2025-05-26 19:43:06 - model.transformer - INFO - Using flow distribution mode: direct
2025-05-26 19:43:06 - model.transformer - INFO - Using standard attention with flash attention: True
2025-05-26 19:43:06 - model.transformer - INFO - GMMTransformer created with 197,184 parameters
2025-05-26 19:43:06 - model.factory - INFO - Created cluster prediction model with 197,440 parameters
2025-05-26 19:43:07 - training.trainer - INFO - TensorBoard logging enabled at output/test/test_20250526_194306/tensorboard
2025-05-26 19:43:07 - training.experiment - INFO - Experiment setup completed
2025-05-26 19:43:07 - training.trainer - INFO - Starting training for 1 epochs (from epoch 1 to 1)
2025-05-26 19:43:38 - config.base - INFO - Configuration loaded from output/final_experiments/simple_16_layers/config.json
2025-05-26 19:43:38 - model.transformer - INFO - Using flow distribution mode: direct
2025-05-26 19:43:38 - model.transformer - INFO - Using standard attention with flash attention: True
2025-05-26 19:43:38 - model.transformer - INFO - Using layerwise repetition mode with 16 effective layers
2025-05-26 19:43:38 - model.transformer - INFO - Unique layers: 1, repeat factor: 16
2025-05-26 19:43:38 - model.transformer - INFO - Flow distribution mode: direct
2025-05-26 19:43:38 - model.transformer - INFO - GMMTransformer created with 196,992 parameters
2025-05-26 19:43:38 - model.factory - INFO - Created cluster prediction model with 197,504 parameters
2025-05-26 19:43:38 - config.base - INFO - Configuration loaded from output/final_experiments/hard_16_layers/config.json
2025-05-26 19:43:38 - model.transformer - INFO - Using flow distribution mode: direct
2025-05-26 19:43:38 - model.transformer - INFO - Using standard attention with flash attention: True
2025-05-26 19:43:38 - model.transformer - INFO - Using layerwise repetition mode with 16 effective layers
2025-05-26 19:43:38 - model.transformer - INFO - Unique layers: 1, repeat factor: 16
2025-05-26 19:43:38 - model.transformer - INFO - Flow distribution mode: direct
2025-05-26 19:43:38 - model.transformer - INFO - GMMTransformer created with 196,992 parameters
2025-05-26 19:43:38 - model.factory - INFO - Created cluster prediction model with 197,504 parameters
2025-05-26 19:43:45 - config.base - INFO - Configuration loaded from output/final_experiments/no_flow_16_layers/config.json
2025-05-26 19:43:45 - model.transformer - INFO - Using flow distribution mode: direct
2025-05-26 19:43:45 - model.transformer - INFO - Using standard attention with flash attention: True
2025-05-26 19:43:45 - model.transformer - INFO - Using layerwise repetition mode with 16 effective layers
2025-05-26 19:43:45 - model.transformer - INFO - Unique layers: 1, repeat factor: 16
2025-05-26 19:43:45 - model.transformer - INFO - Flow distribution mode: direct
2025-05-26 19:43:45 - model.transformer - INFO - GMMTransformer created with 196,992 parameters
2025-05-26 19:43:45 - model.factory - INFO - Created cluster prediction model with 197,504 parameters
2025-05-26 19:44:07 - matplotlib.animation - INFO - Animation.save using <class 'matplotlib.animation.FFMpegWriter'>
2025-05-26 19:44:07 - matplotlib.animation - INFO - MovieWriter._run: running command: ffmpeg -f rawvideo -vcodec rawvideo -s 1600x800 -pix_fmt rgba -framerate 20 -loglevel error -i pipe: -filter_complex 'split [a][b];[a] palettegen [p];[b][p] paletteuse' -metadata artist=GMM-Tutorial -y /var/folders/s4/twbqddyx3jx952l_6bcxn1900000gn/T/tmpcp5459n6/flow_substitution_animation.gif
