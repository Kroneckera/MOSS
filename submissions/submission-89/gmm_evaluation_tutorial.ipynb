{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GMM Model Evaluation Tutorial"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1: Introduction and Setup\n",
    "\n",
    "In this first part, we'll prepare our environment by importing essential libraries and configuring paths for smooth project navigation. This setup ensures reproducibility and consistency when evaluating Gaussian Mixture Model (GMM) implementations.\n",
    "\n",
    "Let's start by importing all the necessary utilities and defining our project structure:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import torch\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from pathlib import Path\n",
    "\n",
    "# Add project root to path if needed\n",
    "project_root = Path('.')\n",
    "if project_root not in sys.path:\n",
    "    sys.path.insert(0, str(project_root))\n",
    "output_dir = project_root / 'output'\n",
    "experiment_base_dir = output_dir / 'final_experiments'\n",
    "\n",
    "# Set device\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "print(f\"Device: {device}\")\n",
    "\n",
    "import logging\n",
    "logging.basicConfig(level=logging.WARNING)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Import Evaluation and Visualization Utilities\n",
    "\n",
    "Next, we'll import specific functions required for evaluating and visualizing our Gaussian Mixture Models. These utilities are organized into three main categories:\n",
    "\n",
    "- **Data Handling** (`io`): Loading models and data loaders.\n",
    "- **Evaluation** (`eval_utils`): Evaluating model performance, running baseline comparisons like K-means, and computing metrics.\n",
    "- **Visualization** (`visualization`): Tools for plotting, formatting figures, and creating animations for insightful comparisons."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scripts.evaluation.tutorial.src.io import (\n",
    "    load_model_from_experiment,\n",
    "    create_data_loader\n",
    ")\n",
    "from scripts.evaluation.tutorial.src.eval_utils import (\n",
    "    evaluate, \n",
    "    evaluate_dataset,\n",
    "    run_kmeans,\n",
    "    compute_metrics,\n",
    "    get_flow_prediction\n",
    ")\n",
    "from scripts.evaluation.tutorial.src.visualization import (\n",
    "    visualize_gmm_data,\n",
    "    set_plotting_style,\n",
    "    save_figure,\n",
    "    create_comparison_grid,\n",
    "    create_comparison_figure,\n",
    "    format_axis_with_grid,\n",
    "    format_legend,\n",
    "    save_animation,\n",
    "    VisualizationPipeline\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 2: Dataset Comparison\n",
    "\n",
    "In this section, we will visualize and compare different GMM datasets to better understand how variations in dataset characteristics (such as cluster separability and complexity) affect model performance.\n",
    "\n",
    "We'll start by initializing our visualization pipeline, which manages data handling and plotting in a convenient way."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the visualization pipeline\n",
    "import tempfile\n",
    "temp_dir = Path(tempfile.mkdtemp())  # Create temporary directory for pipeline\n",
    "pipeline = VisualizationPipeline(\n",
    "    experiment_dir=experiment_base_dir,  # Use the already defined experiment directory\n",
    "    output_dir=temp_dir,\n",
    "    device=device,\n",
    "    verbose=False  # Enable verbose output for progress tracking\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Basic Dataset Comparison\n",
    "\n",
    "We'll generate and visualize examples from three types of synthetic Gaussian Mixture Model (GMM) datasets, each containing 1000 samples per instance:\n",
    "\n",
    "- **Simple Dataset**: High separability with SNR-dB values sampled from a truncated normal distribution (mean=14.0, std=1.5, min=12.0, max=17.0). Number of clusters sampled from truncated Poisson (mean=2.5, min=2, max=4).\n",
    "\n",
    "- **Standard Dataset**: Moderate complexity with SNR-dB values from a truncated normal distribution (mean=9.0, std=1.5, min=7.0, max=12.0). Number of clusters sampled from truncated Poisson (mean=5.0, min=3, max=7).\n",
    "\n",
    "- **Complex Dataset**: High complexity with SNR-dB values from a truncated normal distribution (mean=5.0, std=1.5, min=3.0, max=7.0). Number of clusters sampled from truncated Poisson (mean=8.0, min=5, max=15).\n",
    "\n",
    "We'll visualize three samples from each dataset type to illustrate how cluster separation varies across these scenarios."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate 3 unique samples per dataset\n",
    "results = []\n",
    "dataset_types = ['simple', 'standard', 'complex']\n",
    "dataset_labels = ['Simple', 'Standard', 'Complex']\n",
    "\n",
    "# For each dataset type, generate 3 unique samples\n",
    "for dataset_type, label in zip(dataset_types, dataset_labels):\n",
    "    dataset_results = pipeline._process_dataset_input(\n",
    "        datasets=dataset_type,\n",
    "        models=None,\n",
    "        parameter_values=None,\n",
    "        show=['points', 'true_centers'],\n",
    "        num_samples=3\n",
    "    )\n",
    "    \n",
    "    # Add proper titles to the results\n",
    "    for i, result in enumerate(dataset_results):\n",
    "        result['metadata']['title'] = f\"{label} Sample {i+1}\"\n",
    "        results.append(result)\n",
    "\n",
    "# Create the grid\n",
    "titles = [result['metadata']['title'] for result in results]\n",
    "fig, axes = create_comparison_grid(\n",
    "    results=results,\n",
    "    layout='3x3',\n",
    "    show_predictions=False,\n",
    "    show_kmeans=False,\n",
    "    titles=titles,\n",
    "    figsize=(10, 10),\n",
    "    verbose=False,\n",
    "    size_scale=0.6\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SNR Dataset Comparison\n",
    "\n",
    "We'll now visualize Gaussian Mixture Model datasets with varying noise levels (SNR), each containing a variable sample size between 500 and 2000 samples per instance. Each dataset maintains moderate cluster complexity, with the number of clusters sampled from a truncated Poisson distribution (mean=5.0, min=3, max=7):\n",
    "\n",
    "- **High-SNR Dataset**: Fixed high SNR-dB at 15, very clear cluster separation.\n",
    "- **Medium-SNR Dataset**: Fixed medium SNR-dB at 10, moderate cluster visibility.\n",
    "- **Low-SNR Dataset**: Fixed low SNR-dB at 5, significant cluster overlap.\n",
    "\n",
    "We'll generate three identical examples (with different SNR) from each dataset type to highlight how different SNR levels impact data visibility and clustering difficulty.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate 3 unique samples per dataset\n",
    "results = []\n",
    "dataset_types = ['high_snr_fixed', 'average_snr_fixed', 'low_snr_fixed']\n",
    "dataset_labels = ['High SNR', 'Moderate SNR', 'Low SNR']\n",
    "\n",
    "# For each dataset type, generate 3 unique samples\n",
    "for dataset_type, label in zip(dataset_types, dataset_labels):\n",
    "    dataset_results = pipeline._process_dataset_input(\n",
    "        datasets=dataset_type,\n",
    "        models=None,\n",
    "        parameter_values=None,\n",
    "        show=['points', 'true_centers'],\n",
    "        num_samples=3\n",
    "    )\n",
    "    \n",
    "    # Add proper titles to the results\n",
    "    for i, result in enumerate(dataset_results):\n",
    "        result['metadata']['title'] = f\"{label} Sample {i+1}\"\n",
    "        results.append(result)\n",
    "\n",
    "# Create the grid\n",
    "titles = [result['metadata']['title'] for result in results]\n",
    "fig2, axes = create_comparison_grid(\n",
    "    results=results,\n",
    "    layout='3x3',\n",
    "    show_predictions=False,\n",
    "    show_kmeans=False,\n",
    "    titles=titles,\n",
    "    figsize=(10, 10),\n",
    "    verbose=False,\n",
    "    size_scale=0.6\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset Comparison with KMeans Analysis\n",
    "\n",
    "We'll now visualize how KMeans clustering performs as a baseline method on the previously defined datasets:\n",
    "\n",
    "- **Simple**\n",
    "- **Standard**\n",
    "- **Complex**\n",
    "\n",
    "Each plot illustrates the dataset points, true cluster centers (green stars), and cluster centers identified by KMeans (orange diamonds). This helps us intuitively assess how well KMeans recovers cluster structures at different levels of dataset complexity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig4 = pipeline.scatter_plot(\n",
    "    datasets=['simple', 'standard', 'complex'],\n",
    "    show=['points', 'true_centers', 'kmeans'],\n",
    "    layout='1x3',\n",
    "    titles=['Simple + KMeans', 'Standard + KMeans', 'Complex + KMeans'],\n",
    "    figsize=(10, 3.33),\n",
    "    save_path=None,\n",
    "    verbose=False,\n",
    "    size_scale=0.6\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 4: Training a Model\n",
    "\n",
    "Here's how to train a GMM model using the experiment runner. This example shows training a 16-layer model with the same configuration used for the baseline models in the tutorial."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Direct training using ExperimentManager\n",
    "import json\n",
    "from datetime import datetime\n",
    "from config import ExperimentConfig\n",
    "from config.registry import (\n",
    "    get_model_config,\n",
    "    get_training_config,\n",
    "    get_data_config,\n",
    "    get_validation_config\n",
    ")\n",
    "from training.experiment import ExperimentManager\n",
    "\n",
    "def create_and_train_model(\n",
    "    experiment_name=\"tutorial_model\",\n",
    "    epochs=10,\n",
    "    device=\"cuda:0\",\n",
    "    output_dir=\"./output/tutorial_experiments\"\n",
    "):\n",
    "    \"\"\"\n",
    "    Create and train a GMM model using the same configuration as baseline models.\n",
    "    \n",
    "    Args:\n",
    "        experiment_name: Name for the experiment\n",
    "        epochs: Number of training epochs\n",
    "        device: Device to train on\n",
    "        output_dir: Where to save the model\n",
    "    \"\"\"\n",
    "    \n",
    "    # Get preset configurations\n",
    "    model_config = get_model_config(\"medium\")\n",
    "    training_config = get_training_config(\"standard\")\n",
    "    data_config = get_data_config(\"diverse_clusters_snr\")\n",
    "    validation_config = get_validation_config(\"standard\")\n",
    "    \n",
    "    # Build configuration dictionary\n",
    "    config_dict = {\n",
    "        \"model\": model_config.model_dump(),\n",
    "        \"training\": training_config.model_dump(),\n",
    "        \"data\": data_config.model_dump(),\n",
    "        \"validation\": validation_config.model_dump(),\n",
    "        \"metadata\": {\n",
    "            \"id\": f\"{experiment_name}_{datetime.now().strftime('%Y%m%d_%H%M%S')}\",\n",
    "            \"experiment_name\": experiment_name,\n",
    "            \"description\": f\"Tutorial experiment with 16-layer model trained for {epochs} epochs\"\n",
    "        },\n",
    "        \"device\": {\n",
    "            \"device\": device,\n",
    "            \"use_mixed_precision\": False\n",
    "        },\n",
    "        \"paths\": {\n",
    "            \"base_dir\": output_dir,\n",
    "            \"log_dir\": \"logs\",\n",
    "            \"checkpoint_dir\": \"checkpoints\",\n",
    "            \"data_dir\": \"data\"\n",
    "        }\n",
    "    }\n",
    "    \n",
    "    # Configure model for 16 layers (1 layer repeated 16 times)\n",
    "    config_dict[\"model\"][\"transformer\"][\"num_layers\"] = 1\n",
    "    config_dict[\"model\"][\"transformer\"][\"repeat_factor\"] = 16\n",
    "    config_dict[\"model\"][\"transformer\"][\"layer_repeat_mode\"] = \"layerwise\"\n",
    "    \n",
    "    # Configure flow predictor\n",
    "    config_dict[\"model\"][\"transformer\"][\"use_flow_predictor\"] = True\n",
    "    config_dict[\"model\"][\"transformer\"][\"flow_predictor\"] = {\n",
    "        \"type\": \"monotonic\",\n",
    "        \"num_basis\": 100,\n",
    "        \"min_value\": 0.0,\n",
    "        \"max_value\": 1.0,\n",
    "        \"per_layer\": False,\n",
    "        \"min_snr\": 3.0,\n",
    "        \"max_snr\": 15.0,\n",
    "        \"distribution_mode\": \"direct\"\n",
    "    }\n",
    "    \n",
    "    # Configure encoder/decoder\n",
    "    config_dict[\"model\"][\"encoder\"][\"use_orthogonal\"] = True\n",
    "    config_dict[\"model\"][\"decoder\"][\"use_orthogonal\"] = True\n",
    "    \n",
    "    # Configure loss\n",
    "    config_dict[\"training\"][\"loss\"][\"loss_type\"] = {\n",
    "        \"type\": \"wasserstein\",\n",
    "        \"algorithm\": \"exact\",\n",
    "        \"backend\": \"pot\",\n",
    "        \"use_true_weights\": False\n",
    "    }\n",
    "    config_dict[\"training\"][\"loss\"][\"normalization\"] = \"snr_power\"\n",
    "    config_dict[\"training\"][\"loss\"][\"snr_power\"] = 1.0\n",
    "    \n",
    "    # Set number of epochs\n",
    "    config_dict[\"training\"][\"num_epochs\"] = epochs\n",
    "    \n",
    "    # Create and validate configuration\n",
    "    config = ExperimentConfig.model_validate(config_dict)\n",
    "    \n",
    "    print(f\"Creating experiment: {experiment_name}\")\n",
    "    print(f\"Model: 16 layers (1 layer repeated 16 times)\")\n",
    "    print(f\"Training for {epochs} epochs on {device}\")\n",
    "    print(f\"Output directory: {output_dir}/{config.metadata.id}\")\n",
    "    \n",
    "    # Create experiment manager\n",
    "    experiment = ExperimentManager(config)\n",
    "    \n",
    "    # Setup experiment (creates directories, initializes model, etc.)\n",
    "    print(\"\\nSetting up experiment...\")\n",
    "    experiment.setup()\n",
    "    \n",
    "    # Run training\n",
    "    print(\"\\nStarting training...\")\n",
    "    history = experiment.run()\n",
    "    \n",
    "    print(f\"\\nTraining completed!\")\n",
    "    if 'val_loss' in history and len(history['val_loss']) > 0:\n",
    "        print(f\"Final validation loss: {history['val_loss'][-1]:.4f}\")\n",
    "    else:\n",
    "        print(f\"Final training loss: {history['train_loss'][-1]:.4f}\")\n",
    "    print(f\"Model saved to: {experiment.experiment_dir}\")\n",
    "    \n",
    "    return experiment, history\n",
    "\n",
    "# Example: Train a model for 10 epochs\n",
    "# Uncomment to run:\n",
    "# experiment, history = create_and_train_model(\n",
    "#     experiment_name=\"my_tutorial_model\",\n",
    "#     epochs=10,\n",
    "#     device=device  # Uses the device defined earlier in the notebook\n",
    "# )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Quick Training Example\n",
    "\n",
    "Here's how to train a smaller model for testing (fewer epochs, smaller configuration):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Quick test: Train a small model for 3 epochs\n",
    "def train_test_model():\n",
    "    \"\"\"Train a small test model to verify everything works.\"\"\"\n",
    "    \n",
    "    # Get configurations\n",
    "    model_config = get_model_config(\"small\")  # Smaller model for faster training\n",
    "    training_config = get_training_config(\"quick\")  # Quick training preset\n",
    "    data_config = get_data_config(\"standard\")  # Standard data\n",
    "    validation_config = get_validation_config(\"minimal\")  # Minimal validation\n",
    "    \n",
    "    # Create simple config\n",
    "    config_dict = {\n",
    "        \"model\": model_config.model_dump(),\n",
    "        \"training\": training_config.model_dump(),\n",
    "        \"data\": data_config.model_dump(),\n",
    "        \"validation\": validation_config.model_dump(),\n",
    "        \"metadata\": {\n",
    "            \"id\": f\"test_{datetime.now().strftime('%Y%m%d_%H%M%S')}\",\n",
    "            \"experiment_name\": \"quick_test\",\n",
    "            \"description\": \"Quick test model\"\n",
    "        },\n",
    "        \"device\": {\n",
    "            \"device\": device,\n",
    "            \"use_mixed_precision\": False\n",
    "        },\n",
    "        \"paths\": {\n",
    "            \"base_dir\": \"./output/test\",\n",
    "            \"log_dir\": \"logs\",\n",
    "            \"checkpoint_dir\": \"checkpoints\",\n",
    "            \"data_dir\": \"data\"\n",
    "        }\n",
    "    }\n",
    "    \n",
    "    # Simple 4-layer model\n",
    "    config_dict[\"model\"][\"transformer\"][\"num_layers\"] = 4\n",
    "    config_dict[\"training\"][\"num_epochs\"] = 3\n",
    "    \n",
    "    # Disable validation by setting val_every > num_epochs\n",
    "    config_dict[\"training\"][\"val_every\"] = 1000  # Much larger than 3 epochs\n",
    "    \n",
    "    # Create experiment\n",
    "    config = ExperimentConfig.model_validate(config_dict)\n",
    "    experiment = ExperimentManager(config)\n",
    "    \n",
    "    print(\"Training small test model (4 layers, 3 epochs)...\")\n",
    "    print(\"Validation disabled for faster training demo\")\n",
    "    experiment.setup()\n",
    "    \n",
    "    # Run training and capture the history\n",
    "    history = experiment.run()\n",
    "    \n",
    "    # Return both experiment and history\n",
    "    return experiment, history\n",
    "\n",
    "# Uncomment to run a quick test:\n",
    "# experiment, history = train_test_model()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run Training\n",
    "\n",
    "Let's actually train a small model as a demonstration. This will train a 4-layer model for 3 epochs, which should complete quickly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Actually run the training\n",
    "print(\"Starting model training...\")\n",
    "print(\"This will train a small 4-layer model for 3 epochs as a demonstration.\")\n",
    "print(\"Validation is disabled for faster training.\\n\")\n",
    "\n",
    "# Train the model and get both experiment and history\n",
    "test_experiment, history = train_test_model()\n",
    "\n",
    "# Plot the training history\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.figure(figsize=(10, 4))\n",
    "\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(history['train_loss'], label='Train Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.title('Training Progress')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "if 'learning_rate' in history:\n",
    "    plt.plot(history['learning_rate'])\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Learning Rate')\n",
    "    plt.title('Learning Rate Schedule')\n",
    "    plt.grid(True)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(f\"\\n✓ Model trained successfully!\")\n",
    "print(f\"✓ Saved to: {test_experiment.experiment_dir}\")\n",
    "print(f\"✓ Final training loss: {history['train_loss'][-1]:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 6: Model Training and Evaluation: High vs. Low SNR\n",
    "\n",
    "We'll investigate how training the same model architecture on datasets with different Signal-to-Noise Ratios (SNR) impacts its performance. Specifically, we'll:\n",
    "\n",
    "1. Train one model on a **High SNR** dataset and another on a **Low SNR** dataset.\n",
    "2. Evaluate each trained model on both **High** and **Low SNR** datasets.\n",
    "\n",
    "To ensure reproducibility, we'll generate dataset samples using fixed seeds as defined below.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fixed seeds for reproducible results\n",
    "FIXED_SEEDS = {\n",
    "    'high_snr_fixed': 90,   # → 15.0 dB\n",
    "    'low_snr_fixed': 123    # → 5.0 dB\n",
    "}\n",
    "\n",
    "# Generate samples with fixed seeds\n",
    "from scripts.evaluation.tutorial.src.io import create_data_samples\n",
    "\n",
    "samples = {}\n",
    "for dataset_name, seed in FIXED_SEEDS.items():\n",
    "    inputs, targets = create_data_samples(\n",
    "        dataset_name=dataset_name,\n",
    "        num_samples=1,\n",
    "        points_per_gmm=1000,\n",
    "        device=device,\n",
    "        base_seed=seed,\n",
    "        loader_id=f\"{dataset_name}_{seed}\"\n",
    "    )\n",
    "    \n",
    "    samples[dataset_name] = {\n",
    "        'points': inputs[0],\n",
    "        'centers': targets['centers'][0],\n",
    "        'labels': targets['labels'][0],\n",
    "        'snr_db': targets['snr_db'][0].item()\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare simple and hard models on fixed samples\n",
    "models_to_compare = ['simple_16_layers', 'hard_16_layers']\n",
    "results = []\n",
    "\n",
    "for model_name in models_to_compare:\n",
    "    for dataset_name, label in zip(['high_snr_fixed', 'low_snr_fixed'], ['High SNR', 'Low SNR']):\n",
    "        sample = samples[dataset_name]\n",
    "        \n",
    "        result = pipeline._process_single_data_input(\n",
    "            data=sample,\n",
    "            models=model_name,\n",
    "            parameter_values=None,\n",
    "            show=['points', 'predictions'],\n",
    "        )\n",
    "        \n",
    "        result['metadata']['title'] = f\"{model_name}\\n{label} ({sample['snr_db']:.1f} dB)\"\n",
    "        results.append(result)\n",
    "\n",
    "# Create comparison grid\n",
    "for result in results:\n",
    "    result['targets'] = {k: v for k, v in result['targets'].items() if k != 'centers'}\n",
    "titles = [result['metadata']['title'] for result in results]\n",
    "fig, axes = create_comparison_grid(\n",
    "    results=results,\n",
    "    layout='2x2',\n",
    "    show_predictions=True,\n",
    "    show_kmeans=False,\n",
    "    titles=titles,\n",
    "    figsize=(10, 10),\n",
    "    size_scale=0.6,\n",
    "    verbose=False\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 7: Flow Substitution Animation\n",
    "\n",
    "In this section, we demonstrate the effect of manually substituting a fixed flow speed value into a model originally trained without an explicit flow predictor."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Static Flow Comparison\n",
    "\n",
    "First, we'll visualize this substitution to observe how introducing a fixed flow speed directly affects the model's behavior. Specifically, we'll:\n",
    "\n",
    "- Take a baseline model trained without flow adjustment (\"no flow\").\n",
    "- Manually set the flow speed to a fixed value (flow_speed = 0.2), without retraining or changing model parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create static comparison of no_flow vs manual flow injection\n",
    "static_fig = pipeline.create_static_flow_comparison(\n",
    "    base_model='no_flow_16_layers',\n",
    "    flow_value=0.2,\n",
    "    comparison_type='no_flow_vs_manual',\n",
    "    save_path=None  # Display inline instead of saving\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Basic Flow Animation (0 → 1)\n",
    "\n",
    "Next, we'll visualize the impact of substituting a range of flow speeds (from 0 to 1) into a model that was trained without any explicit flow predictor.  \n",
    "\n",
    "This animation smoothly transitions through different manually-set flow speeds, clearly illustrating how changing flow speed alone influences the model's clustering behavior."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create flow substitution animation\n",
    "import os\n",
    "from IPython.display import Image, display\n",
    "\n",
    "# Save animation to a path\n",
    "animation_path = temp_dir / \"flow_substitution_animation.gif\"\n",
    "basic_anim = pipeline.create_flow_substitution_animation(\n",
    "    base_model='no_flow_16_layers',\n",
    "    flow_range=(0.0, 1.0),\n",
    "    frames=100,  # Fewer frames for notebook display\n",
    "    comparison_type='no_flow_vs_manual',\n",
    "    save_path=animation_path,\n",
    "    show_animation=False  # Don't show in separate window\n",
    ")\n",
    "\n",
    "# Display the GIF in the notebook\n",
    "if animation_path.exists():\n",
    "    display(Image(filename=str(animation_path)))\n",
    "else:\n",
    "    print(\"Animation not created\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extended Flow Animation (0 → 5)\n",
    "\n",
    "Now, let's explore an extended range of flow speeds (from 0 to 5). We'll demonstrate two different approaches to achieve this increased flow effect:\n",
    "\n",
    "1. **Direct Flow Adjustment**:  \n",
    "   Keep the original model architecture fixed, substituting flow speeds ranging from 0 to 5.\n",
    "\n",
    "2. **Layer Scaling Approach**:  \n",
    "   Increase the number of layers by a factor of 5 and proportionally reduce the flow speed range to 0–1, effectively simulating the same cumulative flow.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extended range animation with regime comparison\n",
    "extended_animation_path = temp_dir / \"flow_comparison_0_to_5.gif\"\n",
    "custom_anim = pipeline.create_flow_substitution_animation(\n",
    "    base_model='no_flow_16_layers',\n",
    "    flow_range=(0.0, 5.0),\n",
    "    frames=300,  # More frames for smoother animation\n",
    "    comparison_type='regime_comparison',\n",
    "    regime_settings={\n",
    "        'left': {'repeat_factor': 16, 'flow_divisor': 1},    # 1x layers, full flow\n",
    "        'right': {'repeat_factor': 80, 'flow_divisor': 5}    # 5x layers, flow÷5\n",
    "    },\n",
    "    save_path=extended_animation_path,\n",
    "    show_animation=False\n",
    ")\n",
    "\n",
    "# Display the GIF\n",
    "if extended_animation_path.exists():\n",
    "    display(Image(filename=str(extended_animation_path)))\n",
    "else:\n",
    "    print(\"Animation not created\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Uniform vs. Unit Flow Speed Comparison\n",
    "\n",
    "We'll compare two different regimes for distributing flow speed across transformer layers:\n",
    "\n",
    "- **Uniform regime**: Flow speed is uniformly distributed across all layers.\n",
    "- **Unit regime**: Flow speed is allocated entirely to a single layer at a time (fractional approach).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Direct vs Fractional flow mode comparison\n",
    "direct_frac_path = temp_dir / \"direct_vs_fractional.gif\"\n",
    "frac_anim = pipeline.create_flow_substitution_animation(\n",
    "    base_model='no_flow_16_layers',\n",
    "    flow_range=(0.0, 1.0),\n",
    "    frames=300,\n",
    "    comparison_type='direct_vs_fractional',\n",
    "    save_path=direct_frac_path,\n",
    "    show_animation=False\n",
    ")\n",
    "# Display the GIF\n",
    "if direct_frac_path.exists():\n",
    "    display(Image(filename=str(direct_frac_path)))\n",
    "else:\n",
    "    print(\"Animation not created\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Simple vs. Hard Model Comparison\n",
    "\n",
    "In this section, we directly compare two transformer models trained on datasets with different complexities:\n",
    "\n",
    "- **Simple model**: Trained on clearly separable (high-SNR) data.\n",
    "- **Hard model**: Trained on challenging, overlapping (low-SNR) data.\n",
    "\n",
    "We substitute a manual flow speed parameter ranging from 0 to 1 into both models to illustrate how their learned parameters respond differently to changes in flow speed.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper function for model comparisons\n",
    "def create_model_comparison_animation(pipeline, left_model_name, right_model_name, \n",
    "                                    left_name, right_name, flow_range, frames, \n",
    "                                    save_path, **model_settings):\n",
    "    \"\"\"Create side-by-side model comparison animation.\"\"\"\n",
    "    \n",
    "    # Load models\n",
    "    left_model, _ = pipeline._load_model(left_model_name)\n",
    "    right_model, _ = pipeline._load_model(right_model_name)\n",
    "    \n",
    "    # Apply settings\n",
    "    if 'left' in model_settings:\n",
    "        for attr, value in model_settings['left'].items():\n",
    "            if attr != 'model' and hasattr(left_model.transformer, attr):\n",
    "                setattr(left_model.transformer, attr, value)\n",
    "    \n",
    "    if 'right' in model_settings:\n",
    "        for attr, value in model_settings['right'].items():\n",
    "            if attr != 'model' and hasattr(right_model.transformer, attr):\n",
    "                setattr(right_model.transformer, attr, value)\n",
    "    \n",
    "    # Generate flow values\n",
    "    flow_values = np.linspace(flow_range[0], flow_range[1], frames)\n",
    "    \n",
    "    # Create frame data\n",
    "    frame_data = []\n",
    "    for flow_speed in flow_values:\n",
    "        left_flow = flow_speed\n",
    "        right_flow = flow_speed\n",
    "        \n",
    "        if 'left' in model_settings and 'flow_divisor' in model_settings['left']:\n",
    "            left_flow = flow_speed / model_settings['left']['flow_divisor']\n",
    "        if 'right' in model_settings and 'flow_divisor' in model_settings['right']:\n",
    "            right_flow = flow_speed / model_settings['right']['flow_divisor']\n",
    "        \n",
    "        frame_data.append({\n",
    "            'left_model': left_model,\n",
    "            'right_model': right_model,\n",
    "            'left_flow': left_flow,\n",
    "            'right_flow': right_flow,\n",
    "            'titles': [f\"{left_name}: {flow_speed:.2f}\", f\"{right_name}: {flow_speed:.2f}\"],\n",
    "            'parameter_value': flow_speed\n",
    "        })\n",
    "    \n",
    "    return pipeline._create_side_by_side_animation(\n",
    "        frame_data, \n",
    "        save_path=save_path, \n",
    "        show_animation=False\n",
    "    )\n",
    "\n",
    "# Simple vs Hard comparison\n",
    "simple_hard_path = temp_dir / \"simple_vs_hard_direct.gif\"\n",
    "simple_hard_anim = create_model_comparison_animation(\n",
    "    pipeline=pipeline,\n",
    "    left_model_name='simple_16_layers',\n",
    "    right_model_name='hard_16_layers',\n",
    "    left_name=\"Simple\",\n",
    "    right_name=\"Hard\",\n",
    "    flow_range=(0.0, 1.0),\n",
    "    frames=100,\n",
    "    save_path=simple_hard_path,\n",
    "    left={'flow_distribution_mode': 'direct'},\n",
    "    right={'flow_distribution_mode': 'direct'}\n",
    ")\n",
    "\n",
    "# Display the GIF\n",
    "if simple_hard_path.exists():\n",
    "    display(Image(filename=str(simple_hard_path)))\n",
    "else:\n",
    "    print(\"Animation not created\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 8: Model Performance Comparison\n",
    "\n",
    "Comprehensive evaluation of different model architectures and configurations across various datasets and SNR levels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "# Configuration\n",
    "datasets = [\"high_snr_fixed\", \"average_snr_fixed\", \"low_snr_fixed\"]\n",
    "batch_size = 32\n",
    "total_samples = 4096\n",
    "\n",
    "# Create cache directory matching original script structure\n",
    "snr_cache_dir = project_root / 'scripts/evaluation/tutorial/output/snr_performance'\n",
    "snr_cache_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# Define models to evaluate\n",
    "snr_model_configs = {\n",
    "    \"baseline_16_layers\": {\"name\": \"16 layers\", \"path\": \"baseline_16_layers\"},\n",
    "    \"baseline_32_layers\": {\"name\": \"32 layers\", \"path\": \"baseline_32_layers\"},\n",
    "    \"baseline_64_layers\": {\"name\": \"64 layers\", \"path\": \"baseline_64_layers\"},\n",
    "    \"no_flow_16_layers\": {\"name\": \"No flow (16 layers)\", \"path\": \"no_flow_16_layers\"},\n",
    "}\n",
    "\n",
    "def get_snr_cache_path(dataset_name, model_key=None):\n",
    "    \"\"\"Get cache file path for SNR evaluation results.\"\"\"\n",
    "    if model_key:\n",
    "        return snr_cache_dir / f\"{dataset_name}_{model_key}_results.pt\"\n",
    "    else:\n",
    "        return snr_cache_dir / f\"{dataset_name}_kmeans_results.pt\"\n",
    "\n",
    "def evaluate_model_on_dataset(model_key, model_config, dataset_name, device):\n",
    "    \"\"\"Evaluate a single model on a dataset, with caching.\"\"\"\n",
    "    cache_path = get_snr_cache_path(dataset_name, model_key)\n",
    "    \n",
    "    # Try to load cached results\n",
    "    if cache_path.exists():\n",
    "        return torch.load(cache_path, weights_only=False)\n",
    "    \n",
    "    print(f\"  Evaluating {model_config['name']} on {dataset_name}...\")\n",
    "    model_path = experiment_base_dir / model_config[\"path\"]\n",
    "    model, _ = load_model_from_experiment(model_path, load_best=False, device=device)\n",
    "    \n",
    "    # Create data loader\n",
    "    data_loader = create_data_loader(\n",
    "        dataset_name=dataset_name,\n",
    "        batch_size=batch_size,\n",
    "        total_samples=total_samples,\n",
    "        device=device\n",
    "    )\n",
    "    \n",
    "    # Evaluate dataset\n",
    "    eval_results_list = evaluate_dataset(\n",
    "        model, \n",
    "        data_loader,\n",
    "        kmeans_on_inputs=False,\n",
    "        kmeans_on_predictions=False,\n",
    "        metrics=['log_wasserstein'],\n",
    "        device=device\n",
    "    )\n",
    "    \n",
    "    # Aggregate results\n",
    "    all_wass = []\n",
    "    all_log_wass = []\n",
    "    all_snr_db = []\n",
    "    \n",
    "    for batch_results in eval_results_list:\n",
    "        if 'metrics' in batch_results and 'log_wasserstein' in batch_results['metrics']:\n",
    "            batch_log_wass = batch_results['metrics']['log_wasserstein'].cpu().numpy()\n",
    "            all_log_wass.extend(batch_log_wass)\n",
    "            batch_wass = np.exp(batch_log_wass)\n",
    "            all_wass.extend(batch_wass)\n",
    "        if 'snr_values' in batch_results and batch_results['snr_values'] is not None:\n",
    "            batch_snr = batch_results['snr_values'].cpu().numpy()\n",
    "            all_snr_db.extend(batch_snr)\n",
    "    \n",
    "    results = {\n",
    "        'wasserstein': np.array(all_wass),\n",
    "        'log_wasserstein': np.array(all_log_wass),\n",
    "        'snr_db': np.array(all_snr_db) if all_snr_db else None,\n",
    "        'avg_wasserstein': np.mean(all_wass),\n",
    "        'std_wasserstein': np.std(all_wass),\n",
    "        'avg_log_wasserstein': np.mean(all_log_wass),\n",
    "        'std_log_wasserstein': np.std(all_log_wass)\n",
    "    }\n",
    "    \n",
    "    # Cache results\n",
    "    torch.save(results, cache_path)\n",
    "    return results\n",
    "\n",
    "def evaluate_kmeans_baseline(dataset_name, device):\n",
    "    \"\"\"Evaluate K-means baseline on a dataset, with caching.\"\"\"\n",
    "    cache_path = get_snr_cache_path(dataset_name)\n",
    "    \n",
    "    # Try to load cached results\n",
    "    if cache_path.exists():\n",
    "        return torch.load(cache_path, weights_only=False)\n",
    "    \n",
    "    data_loader = create_data_loader(\n",
    "        dataset_name=dataset_name,\n",
    "        batch_size=batch_size,\n",
    "        total_samples=total_samples,\n",
    "        device=device\n",
    "    )\n",
    "    \n",
    "    # Use any model just to get K-means results\n",
    "    model_path = experiment_base_dir / \"baseline_16_layers\"\n",
    "    model, _ = load_model_from_experiment(model_path, load_best=False, device=device)\n",
    "    \n",
    "    # Evaluate with K-means only\n",
    "    eval_results_list = evaluate_dataset(\n",
    "        model, \n",
    "        data_loader,\n",
    "        kmeans_on_inputs=True,\n",
    "        kmeans_on_predictions=False,\n",
    "        metrics=['log_kmeans_wasserstein'],\n",
    "        device=device\n",
    "    )\n",
    "    \n",
    "    # Aggregate K-means results\n",
    "    all_kmeans_wass = []\n",
    "    all_log_kmeans_wass = []\n",
    "    all_snr_db = []\n",
    "    \n",
    "    for batch_results in eval_results_list:\n",
    "        if 'metrics' in batch_results and 'log_kmeans_wasserstein' in batch_results['metrics']:\n",
    "            batch_log_kmeans_wass = batch_results['metrics']['log_kmeans_wasserstein'].cpu().numpy()\n",
    "            all_log_kmeans_wass.extend(batch_log_kmeans_wass)\n",
    "            batch_kmeans_wass = np.exp(batch_log_kmeans_wass)\n",
    "            all_kmeans_wass.extend(batch_kmeans_wass)\n",
    "        if 'snr_values' in batch_results and batch_results['snr_values'] is not None:\n",
    "            batch_snr = batch_results['snr_values'].cpu().numpy()\n",
    "            all_snr_db.extend(batch_snr)\n",
    "    \n",
    "    results = {\n",
    "        'wasserstein': np.array(all_kmeans_wass),\n",
    "        'log_wasserstein': np.array(all_log_kmeans_wass),\n",
    "        'snr_db': np.array(all_snr_db) if all_snr_db else None,\n",
    "        'avg_wasserstein': np.mean(all_kmeans_wass),\n",
    "        'std_wasserstein': np.std(all_kmeans_wass),\n",
    "        'avg_log_wasserstein': np.mean(all_log_kmeans_wass),\n",
    "        'std_log_wasserstein': np.std(all_log_kmeans_wass)\n",
    "    }\n",
    "    \n",
    "    # Cache results\n",
    "    torch.save(results, cache_path)\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create average performance plot for SNR analysis\n",
    "def evaluate_models_for_plotting(model_configs, device):\n",
    "    \"\"\"Evaluate all models on diverse_snr_moderate dataset for plotting, with caching.\"\"\"\n",
    "    \n",
    "    diverse_dataset = \"diverse_snr_moderate\"\n",
    "    batch_size = 1\n",
    "    total_samples = 4096\n",
    "    \n",
    "    # Store all results\n",
    "    all_results = {}\n",
    "    \n",
    "    # Evaluate K-means baseline\n",
    "    kmeans_cache_path = snr_cache_dir / f\"{diverse_dataset}_kmeans_full_results.pt\"\n",
    "    \n",
    "    if kmeans_cache_path.exists():\n",
    "        print(f\"Loading cached K-means results from: {kmeans_cache_path}\")\n",
    "        all_results['kmeans'] = torch.load(kmeans_cache_path, weights_only=False)\n",
    "    else:\n",
    "        print(f\"Evaluating K-means baseline on {diverse_dataset}...\")\n",
    "        # Create data loader\n",
    "        data_loader = create_data_loader(\n",
    "            dataset_name=diverse_dataset,\n",
    "            batch_size=batch_size,\n",
    "            total_samples=total_samples,\n",
    "            device=device\n",
    "        )\n",
    "        \n",
    "        # Use any model just to get K-means results\n",
    "        model_path = experiment_base_dir / \"baseline_16_layers\"\n",
    "        model, _ = load_model_from_experiment(model_path, load_best=False, device=device)\n",
    "        \n",
    "        # Evaluate with K-means only\n",
    "        eval_results_list = evaluate_dataset(\n",
    "            model, \n",
    "            data_loader,\n",
    "            kmeans_on_inputs=True,\n",
    "            kmeans_on_predictions=False,\n",
    "            metrics=['log_kmeans_wasserstein'],\n",
    "            device=device\n",
    "        )\n",
    "        \n",
    "        # Aggregate K-means results\n",
    "        all_kmeans_wass = []\n",
    "        all_log_kmeans_wass = []\n",
    "        all_snr_db = []\n",
    "        \n",
    "        for batch_results in eval_results_list:\n",
    "            if 'metrics' in batch_results and 'log_kmeans_wasserstein' in batch_results['metrics']:\n",
    "                batch_log_kmeans_wass = batch_results['metrics']['log_kmeans_wasserstein'].cpu().numpy()\n",
    "                all_log_kmeans_wass.extend(batch_log_kmeans_wass)\n",
    "                batch_kmeans_wass = np.exp(batch_log_kmeans_wass)\n",
    "                all_kmeans_wass.extend(batch_kmeans_wass)\n",
    "            if 'snr_values' in batch_results and batch_results['snr_values'] is not None:\n",
    "                batch_snr = batch_results['snr_values'].cpu().numpy()\n",
    "                all_snr_db.extend(batch_snr)\n",
    "        \n",
    "        kmeans_results = {\n",
    "            'wasserstein': np.array(all_kmeans_wass),\n",
    "            'log_wasserstein': np.array(all_log_kmeans_wass),\n",
    "            'snr_db': np.array(all_snr_db) if all_snr_db else None,\n",
    "            'avg_wasserstein': np.mean(all_kmeans_wass),\n",
    "            'avg_log_wasserstein': np.mean(all_log_kmeans_wass),\n",
    "        }\n",
    "        \n",
    "        all_results['kmeans'] = kmeans_results\n",
    "        # Save the full results with different name to avoid overwriting simple cache\n",
    "        torch.save(kmeans_results, kmeans_cache_path)\n",
    "        print(f\"Saved K-means results to: {kmeans_cache_path}\")\n",
    "    \n",
    "    # Evaluate each model\n",
    "    for model_key, model_config in model_configs.items():\n",
    "        cache_path = snr_cache_dir / f\"{diverse_dataset}_{model_key}_full_results.pt\"\n",
    "        \n",
    "        if cache_path.exists():\n",
    "            all_results[model_key] = torch.load(cache_path, weights_only=False)\n",
    "        else:\n",
    "            print(f\"Evaluating {model_config['name']} on {diverse_dataset}...\")\n",
    "            # Create data loader\n",
    "            data_loader = create_data_loader(\n",
    "                dataset_name=diverse_dataset,\n",
    "                batch_size=batch_size,\n",
    "                total_samples=total_samples,\n",
    "                device=device\n",
    "            )\n",
    "            \n",
    "            # Load model\n",
    "            model_path = experiment_base_dir / model_config[\"path\"]\n",
    "            model, _ = load_model_from_experiment(model_path, load_best=False, device=device)\n",
    "            \n",
    "            # Evaluate dataset\n",
    "            eval_results_list = evaluate_dataset(\n",
    "                model, \n",
    "                data_loader,\n",
    "                kmeans_on_inputs=False,\n",
    "                kmeans_on_predictions=False,\n",
    "                metrics=['log_wasserstein'],\n",
    "                device=device\n",
    "            )\n",
    "            \n",
    "            # Aggregate results\n",
    "            all_wass = []\n",
    "            all_log_wass = []\n",
    "            all_snr_db = []\n",
    "            \n",
    "            for batch_results in eval_results_list:\n",
    "                if 'metrics' in batch_results and 'log_wasserstein' in batch_results['metrics']:\n",
    "                    batch_log_wass = batch_results['metrics']['log_wasserstein'].cpu().numpy()\n",
    "                    all_log_wass.extend(batch_log_wass)\n",
    "                    batch_wass = np.exp(batch_log_wass)\n",
    "                    all_wass.extend(batch_wass)\n",
    "                if 'snr_values' in batch_results and batch_results['snr_values'] is not None:\n",
    "                    batch_snr = batch_results['snr_values'].cpu().numpy()\n",
    "                    all_snr_db.extend(batch_snr)\n",
    "            \n",
    "            model_results = {\n",
    "                'wasserstein': np.array(all_wass),\n",
    "                'log_wasserstein': np.array(all_log_wass),\n",
    "                'snr_db': np.array(all_snr_db) if all_snr_db else None,\n",
    "                'avg_wasserstein': np.mean(all_wass),\n",
    "                'avg_log_wasserstein': np.mean(all_log_wass),\n",
    "            }\n",
    "            \n",
    "            all_results[model_key] = model_results\n",
    "            # Save the full results with different name\n",
    "            torch.save(model_results, cache_path)\n",
    "            print(f\"Saved results to: {cache_path}\")\n",
    "    \n",
    "    return all_results\n",
    "\n",
    "def plot_snr_performance(model_configs):\n",
    "    \"\"\"Plot average performance vs SNR for diverse dataset.\"\"\"\n",
    "    \n",
    "    # Get cached results for diverse dataset\n",
    "    diverse_results = evaluate_models_for_plotting(model_configs, device)\n",
    "    \n",
    "    # Create plot\n",
    "    fig, ax = plt.subplots(figsize=(10, 6))\n",
    "    \n",
    "    # Define SNR bins\n",
    "    snr_min, snr_max = 3.0, 15.0\n",
    "    bin_edges = np.linspace(snr_min, snr_max, 10 + 1)\n",
    "    bin_centers = (bin_edges[:-1] + bin_edges[1:]) / 2\n",
    "    \n",
    "    # Colors for different models\n",
    "    colors = {\n",
    "        'kmeans': 'orange',\n",
    "        'baseline_16_layers': 'blue',\n",
    "        'baseline_32_layers': 'red',\n",
    "        'baseline_64_layers': 'green',\n",
    "        'no_flow_16_layers': 'purple'\n",
    "    }\n",
    "    \n",
    "    # Plot each model\n",
    "    for model_key in ['kmeans'] + list(model_configs.keys()):\n",
    "        if model_key in diverse_results:\n",
    "            log_wass = diverse_results[model_key]['log_wasserstein']\n",
    "            snr_db = diverse_results[model_key]['snr_db']\n",
    "            \n",
    "            # Compute average in each bin\n",
    "            avg_wass = []\n",
    "            for i in range(len(bin_edges) - 1):\n",
    "                mask = (snr_db >= bin_edges[i]) & (snr_db < bin_edges[i + 1])\n",
    "                if np.any(mask):\n",
    "                    avg_wass.append(np.exp(log_wass[mask].mean()))\n",
    "                else:\n",
    "                    avg_wass.append(np.nan)\n",
    "            \n",
    "            avg_wass = np.array(avg_wass)\n",
    "            valid_mask = ~np.isnan(avg_wass)\n",
    "            \n",
    "            if model_key == 'kmeans':\n",
    "                label = 'K-means'\n",
    "            else:\n",
    "                label = model_configs[model_key]['name']\n",
    "            \n",
    "            color = colors.get(model_key, 'black')\n",
    "            ax.plot(bin_centers[valid_mask], avg_wass[valid_mask], \n",
    "                    color=color, marker='o', markersize=6, \n",
    "                    linewidth=2, label=label)\n",
    "    \n",
    "    ax.set_xlabel('SNR (dB)')\n",
    "    ax.set_ylabel('Average Wasserstein Distance')\n",
    "    ax.set_title('Model Performance vs SNR')\n",
    "    ax.set_xlim(snr_min - 0.5, snr_max + 0.5)\n",
    "    ax.set_yscale('log')\n",
    "    ax.grid(True, alpha=0.3)\n",
    "    ax.legend()\n",
    "    \n",
    "    return fig\n",
    "\n",
    "# Create the plot\n",
    "snr_plot = plot_snr_performance(snr_model_configs)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
