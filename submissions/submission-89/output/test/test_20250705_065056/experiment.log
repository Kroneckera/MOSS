2025-07-05 06:50:56 - training.experiment - INFO - Experiment 'quick_test' initialized with ID: test_20250705_065056
2025-07-05 06:50:56 - training.experiment - INFO - Created data loaders with batch sizes: train=16, val=64
2025-07-05 06:50:56 - model.transformer - INFO - Using flow distribution mode: direct
2025-07-05 06:50:56 - model.transformer - INFO - Using standard attention with flash attention: True
2025-07-05 06:50:56 - model.transformer - INFO - GMMTransformer created with 197,184 parameters
2025-07-05 06:50:56 - model.factory - INFO - Created cluster prediction model with 197,440 parameters
2025-07-05 06:50:56 - training.trainer - INFO - TensorBoard logging enabled at output/test/test_20250705_065056/tensorboard
2025-07-05 06:50:56 - training.experiment - INFO - Experiment setup completed
2025-07-05 06:50:56 - training.trainer - INFO - Starting training for 3 epochs (from epoch 1 to 3)
2025-07-05 06:51:27 - training.trainer - INFO - Epoch 1/3 - Train Loss: 0.068538, Train Normalized Loss: 0.068538
2025-07-05 06:51:27 - training.trainer - INFO - Saved training data loader state to output/test/test_20250705_065056/data_state.json
2025-07-05 06:51:27 - training.trainer - INFO - Saved validation data loader state to output/test/test_20250705_065056/val_data_state.json
2025-07-05 06:51:57 - training.trainer - INFO - Epoch 2/3 - Train Loss: 0.034437, Train Normalized Loss: 0.034437
2025-07-05 06:51:57 - training.trainer - INFO - Saved training data loader state to output/test/test_20250705_065056/data_state.json
2025-07-05 06:51:57 - training.trainer - INFO - Saved validation data loader state to output/test/test_20250705_065056/val_data_state.json
2025-07-05 06:52:28 - training.trainer - INFO - Epoch 3/3 - Train Loss: 0.028969, Train Normalized Loss: 0.028969
2025-07-05 06:52:28 - training.trainer - INFO - Saved training data loader state to output/test/test_20250705_065056/data_state.json
2025-07-05 06:52:28 - training.trainer - INFO - Saved validation data loader state to output/test/test_20250705_065056/val_data_state.json
2025-07-05 06:52:28 - training.trainer - INFO - Saved training data loader state to output/test/test_20250705_065056/data_state.json
2025-07-05 06:52:28 - training.trainer - INFO - Saved validation data loader state to output/test/test_20250705_065056/val_data_state.json
2025-07-05 06:52:28 - training.trainer - INFO - Saved final checkpoint
2025-07-05 06:52:28 - training.experiment - INFO - Training completed in 92.18 seconds
2025-07-05 06:52:57 - config.base - INFO - Configuration loaded from output/final_experiments/simple_16_layers/config.json
2025-07-05 06:52:57 - model.transformer - INFO - Using flow distribution mode: direct
2025-07-05 06:52:57 - model.transformer - INFO - Using standard attention with flash attention: True
2025-07-05 06:52:57 - model.transformer - INFO - Using layerwise repetition mode with 16 effective layers
2025-07-05 06:52:57 - model.transformer - INFO - Unique layers: 1, repeat factor: 16
2025-07-05 06:52:57 - model.transformer - INFO - Flow distribution mode: direct
2025-07-05 06:52:57 - model.transformer - INFO - GMMTransformer created with 196,992 parameters
2025-07-05 06:52:57 - model.factory - INFO - Created cluster prediction model with 197,504 parameters
2025-07-05 06:52:57 - config.base - INFO - Configuration loaded from output/final_experiments/hard_16_layers/config.json
2025-07-05 06:52:57 - model.transformer - INFO - Using flow distribution mode: direct
2025-07-05 06:52:57 - model.transformer - INFO - Using standard attention with flash attention: True
2025-07-05 06:52:57 - model.transformer - INFO - Using layerwise repetition mode with 16 effective layers
2025-07-05 06:52:57 - model.transformer - INFO - Unique layers: 1, repeat factor: 16
2025-07-05 06:52:57 - model.transformer - INFO - Flow distribution mode: direct
2025-07-05 06:52:57 - model.transformer - INFO - GMMTransformer created with 196,992 parameters
2025-07-05 06:52:57 - model.factory - INFO - Created cluster prediction model with 197,504 parameters
2025-07-05 06:53:10 - config.base - INFO - Configuration loaded from output/final_experiments/no_flow_16_layers/config.json
2025-07-05 06:53:10 - model.transformer - INFO - Using flow distribution mode: direct
2025-07-05 06:53:10 - model.transformer - INFO - Using standard attention with flash attention: True
2025-07-05 06:53:10 - model.transformer - INFO - Using layerwise repetition mode with 16 effective layers
2025-07-05 06:53:10 - model.transformer - INFO - Unique layers: 1, repeat factor: 16
2025-07-05 06:53:10 - model.transformer - INFO - Flow distribution mode: direct
2025-07-05 06:53:10 - model.transformer - INFO - GMMTransformer created with 196,992 parameters
2025-07-05 06:53:10 - model.factory - INFO - Created cluster prediction model with 197,504 parameters
2025-07-05 06:53:22 - matplotlib.animation - INFO - Animation.save using <class 'matplotlib.animation.PillowWriter'>
2025-07-05 06:53:22 - matplotlib.animation - WARNING - Warning: discarding the 'bbox_inches' argument in 'savefig_kwargs' as it may cause frame size to vary, which is inappropriate for animation.
2025-07-05 06:54:38 - matplotlib.animation - INFO - Animation.save using <class 'matplotlib.animation.PillowWriter'>
2025-07-05 06:54:38 - matplotlib.animation - WARNING - Warning: discarding the 'bbox_inches' argument in 'savefig_kwargs' as it may cause frame size to vary, which is inappropriate for animation.
2025-07-05 06:56:27 - matplotlib.animation - INFO - Animation.save using <class 'matplotlib.animation.PillowWriter'>
2025-07-05 06:56:27 - matplotlib.animation - WARNING - Warning: discarding the 'bbox_inches' argument in 'savefig_kwargs' as it may cause frame size to vary, which is inappropriate for animation.
2025-07-05 06:57:45 - matplotlib.animation - INFO - Animation.save using <class 'matplotlib.animation.PillowWriter'>
2025-07-05 06:57:45 - matplotlib.animation - WARNING - Warning: discarding the 'bbox_inches' argument in 'savefig_kwargs' as it may cause frame size to vary, which is inappropriate for animation.
2025-07-05 06:58:49 - matplotlib.animation - INFO - Animation.save using <class 'matplotlib.animation.PillowWriter'>
2025-07-05 06:58:49 - matplotlib.animation - WARNING - Warning: discarding the 'bbox_inches' argument in 'savefig_kwargs' as it may cause frame size to vary, which is inappropriate for animation.
2025-07-05 07:00:33 - config.base - INFO - Configuration loaded from output/final_experiments/baseline_16_layers/config.json
2025-07-05 07:00:33 - model.transformer - INFO - Using flow distribution mode: direct
2025-07-05 07:00:33 - model.transformer - INFO - Using standard attention with flash attention: True
2025-07-05 07:00:33 - model.transformer - INFO - Using layerwise repetition mode with 16 effective layers
2025-07-05 07:00:33 - model.transformer - INFO - Unique layers: 1, repeat factor: 16
2025-07-05 07:00:33 - model.transformer - INFO - Flow distribution mode: direct
2025-07-05 07:00:33 - model.monotonic_flow - INFO - Num basis: 100
2025-07-05 07:00:33 - model.monotonic_flow - INFO - Initial thetas: tensor([0.0050, 0.0150, 0.0250, 0.0350, 0.0450, 0.0550, 0.0650, 0.0750, 0.0850,
        0.0950, 0.1050, 0.1150, 0.1250, 0.1350, 0.1450, 0.1550, 0.1650, 0.1750,
        0.1850, 0.1950, 0.2050, 0.2150, 0.2250, 0.2350, 0.2450, 0.2550, 0.2650,
        0.2750, 0.2850, 0.2950, 0.3050, 0.3150, 0.3250, 0.3350, 0.3450, 0.3550,
        0.3650, 0.3750, 0.3850, 0.3950, 0.4050, 0.4150, 0.4250, 0.4350, 0.4450,
        0.4550, 0.4650, 0.4750, 0.4850, 0.4950, 0.5050, 0.5150, 0.5250, 0.5350,
        0.5450, 0.5550, 0.5650, 0.5750, 0.5850, 0.5950, 0.6050, 0.6150, 0.6250,
        0.6350, 0.6450, 0.6550, 0.6650, 0.6750, 0.6850, 0.6950, 0.7050, 0.7150,
        0.7250, 0.7350, 0.7450, 0.7550, 0.7650, 0.7750, 0.7850, 0.7950, 0.8050,
        0.8150, 0.8250, 0.8350, 0.8450, 0.8550, 0.8650, 0.8750, 0.8850, 0.8950,
        0.9050, 0.9150, 0.9250, 0.9350, 0.9450, 0.9550, 0.9650, 0.9750, 0.9850,
        0.9950])
2025-07-05 07:00:33 - model.monotonic_flow - INFO - Initial theta_raw: Parameter containing:
tensor([-2.9444, -2.7678, -2.6150, -2.4800, -2.3589, -2.2488, -2.1477, -2.0541,
        -1.9669, -1.8850, -1.8078, -1.7346, -1.6650, -1.5986, -1.5349, -1.4738,
        -1.4149, -1.3581, -1.3031, -1.2498, -1.1981, -1.1477, -1.0986, -1.0507,
        -1.0039, -0.9580, -0.9131, -0.8690, -0.8257, -0.7832, -0.7413, -0.7000,
        -0.6592, -0.6190, -0.5793, -0.5400, -0.5011, -0.4626, -0.4244, -0.3866,
        -0.3490, -0.3116, -0.2744, -0.2375, -0.2007, -0.1640, -0.1274, -0.0910,
        -0.0546, -0.0182,  0.0182,  0.0546,  0.0910,  0.1274,  0.1640,  0.2007,
         0.2375,  0.2744,  0.3116,  0.3490,  0.3866,  0.4244,  0.4626,  0.5011,
         0.5400,  0.5793,  0.6190,  0.6592,  0.7000,  0.7413,  0.7832,  0.8257,
         0.8690,  0.9131,  0.9580,  1.0039,  1.0507,  1.0986,  1.1477,  1.1981,
         1.2498,  1.3031,  1.3581,  1.4149,  1.4738,  1.5349,  1.5986,  1.6650,
         1.7346,  1.8078,  1.8850,  1.9669,  2.0541,  2.1477,  2.2488,  2.3589,
         2.4800,  2.6150,  2.7678,  2.9444], requires_grad=True)
2025-07-05 07:00:33 - model.monotonic_flow - INFO - Initialized MonotonicBasis with 100 basis functions
2025-07-05 07:00:33 - model.monotonic_flow - INFO - Initialized MonotonicFlowPredictor (per_layer=False, num_layers=N/A, SNR range=[3.0, 15.0], flow range=[0.0, 1.0])
2025-07-05 07:00:33 - model.transformer - INFO - Initialized monotonic flow predictor (per_layer=False, num_basis=100, flow_range=[0.0, 1.0])
2025-07-05 07:00:33 - model.transformer - INFO - GMMTransformer created with 197,192 parameters
2025-07-05 07:00:33 - model.factory - INFO - Created cluster prediction model with 197,704 parameters
2025-07-05 07:01:31 - config.base - INFO - Configuration loaded from output/final_experiments/baseline_16_layers/config.json
2025-07-05 07:01:31 - model.transformer - INFO - Using flow distribution mode: direct
2025-07-05 07:01:31 - model.transformer - INFO - Using standard attention with flash attention: True
2025-07-05 07:01:31 - model.transformer - INFO - Using layerwise repetition mode with 16 effective layers
2025-07-05 07:01:31 - model.transformer - INFO - Unique layers: 1, repeat factor: 16
2025-07-05 07:01:31 - model.transformer - INFO - Flow distribution mode: direct
2025-07-05 07:01:31 - model.monotonic_flow - INFO - Num basis: 100
2025-07-05 07:01:31 - model.monotonic_flow - INFO - Initial thetas: tensor([0.0050, 0.0150, 0.0250, 0.0350, 0.0450, 0.0550, 0.0650, 0.0750, 0.0850,
        0.0950, 0.1050, 0.1150, 0.1250, 0.1350, 0.1450, 0.1550, 0.1650, 0.1750,
        0.1850, 0.1950, 0.2050, 0.2150, 0.2250, 0.2350, 0.2450, 0.2550, 0.2650,
        0.2750, 0.2850, 0.2950, 0.3050, 0.3150, 0.3250, 0.3350, 0.3450, 0.3550,
        0.3650, 0.3750, 0.3850, 0.3950, 0.4050, 0.4150, 0.4250, 0.4350, 0.4450,
        0.4550, 0.4650, 0.4750, 0.4850, 0.4950, 0.5050, 0.5150, 0.5250, 0.5350,
        0.5450, 0.5550, 0.5650, 0.5750, 0.5850, 0.5950, 0.6050, 0.6150, 0.6250,
        0.6350, 0.6450, 0.6550, 0.6650, 0.6750, 0.6850, 0.6950, 0.7050, 0.7150,
        0.7250, 0.7350, 0.7450, 0.7550, 0.7650, 0.7750, 0.7850, 0.7950, 0.8050,
        0.8150, 0.8250, 0.8350, 0.8450, 0.8550, 0.8650, 0.8750, 0.8850, 0.8950,
        0.9050, 0.9150, 0.9250, 0.9350, 0.9450, 0.9550, 0.9650, 0.9750, 0.9850,
        0.9950])
2025-07-05 07:01:31 - model.monotonic_flow - INFO - Initial theta_raw: Parameter containing:
tensor([-2.9444, -2.7678, -2.6150, -2.4800, -2.3589, -2.2488, -2.1477, -2.0541,
        -1.9669, -1.8850, -1.8078, -1.7346, -1.6650, -1.5986, -1.5349, -1.4738,
        -1.4149, -1.3581, -1.3031, -1.2498, -1.1981, -1.1477, -1.0986, -1.0507,
        -1.0039, -0.9580, -0.9131, -0.8690, -0.8257, -0.7832, -0.7413, -0.7000,
        -0.6592, -0.6190, -0.5793, -0.5400, -0.5011, -0.4626, -0.4244, -0.3866,
        -0.3490, -0.3116, -0.2744, -0.2375, -0.2007, -0.1640, -0.1274, -0.0910,
        -0.0546, -0.0182,  0.0182,  0.0546,  0.0910,  0.1274,  0.1640,  0.2007,
         0.2375,  0.2744,  0.3116,  0.3490,  0.3866,  0.4244,  0.4626,  0.5011,
         0.5400,  0.5793,  0.6190,  0.6592,  0.7000,  0.7413,  0.7832,  0.8257,
         0.8690,  0.9131,  0.9580,  1.0039,  1.0507,  1.0986,  1.1477,  1.1981,
         1.2498,  1.3031,  1.3581,  1.4149,  1.4738,  1.5349,  1.5986,  1.6650,
         1.7346,  1.8078,  1.8850,  1.9669,  2.0541,  2.1477,  2.2488,  2.3589,
         2.4800,  2.6150,  2.7678,  2.9444], requires_grad=True)
2025-07-05 07:01:31 - model.monotonic_flow - INFO - Initialized MonotonicBasis with 100 basis functions
2025-07-05 07:01:31 - model.monotonic_flow - INFO - Initialized MonotonicFlowPredictor (per_layer=False, num_layers=N/A, SNR range=[3.0, 15.0], flow range=[0.0, 1.0])
2025-07-05 07:01:31 - model.transformer - INFO - Initialized monotonic flow predictor (per_layer=False, num_basis=100, flow_range=[0.0, 1.0])
2025-07-05 07:01:31 - model.transformer - INFO - GMMTransformer created with 197,192 parameters
2025-07-05 07:01:31 - model.factory - INFO - Created cluster prediction model with 197,704 parameters
2025-07-05 07:02:20 - config.base - INFO - Configuration loaded from output/final_experiments/baseline_32_layers/config.json
2025-07-05 07:02:20 - model.transformer - INFO - Using flow distribution mode: direct
2025-07-05 07:02:20 - model.transformer - INFO - Using standard attention with flash attention: True
2025-07-05 07:02:20 - model.transformer - INFO - Using layerwise repetition mode with 32 effective layers
2025-07-05 07:02:20 - model.transformer - INFO - Unique layers: 1, repeat factor: 32
2025-07-05 07:02:20 - model.transformer - INFO - Flow distribution mode: direct
2025-07-05 07:02:20 - model.monotonic_flow - INFO - Num basis: 100
2025-07-05 07:02:20 - model.monotonic_flow - INFO - Initial thetas: tensor([0.0050, 0.0150, 0.0250, 0.0350, 0.0450, 0.0550, 0.0650, 0.0750, 0.0850,
        0.0950, 0.1050, 0.1150, 0.1250, 0.1350, 0.1450, 0.1550, 0.1650, 0.1750,
        0.1850, 0.1950, 0.2050, 0.2150, 0.2250, 0.2350, 0.2450, 0.2550, 0.2650,
        0.2750, 0.2850, 0.2950, 0.3050, 0.3150, 0.3250, 0.3350, 0.3450, 0.3550,
        0.3650, 0.3750, 0.3850, 0.3950, 0.4050, 0.4150, 0.4250, 0.4350, 0.4450,
        0.4550, 0.4650, 0.4750, 0.4850, 0.4950, 0.5050, 0.5150, 0.5250, 0.5350,
        0.5450, 0.5550, 0.5650, 0.5750, 0.5850, 0.5950, 0.6050, 0.6150, 0.6250,
        0.6350, 0.6450, 0.6550, 0.6650, 0.6750, 0.6850, 0.6950, 0.7050, 0.7150,
        0.7250, 0.7350, 0.7450, 0.7550, 0.7650, 0.7750, 0.7850, 0.7950, 0.8050,
        0.8150, 0.8250, 0.8350, 0.8450, 0.8550, 0.8650, 0.8750, 0.8850, 0.8950,
        0.9050, 0.9150, 0.9250, 0.9350, 0.9450, 0.9550, 0.9650, 0.9750, 0.9850,
        0.9950])
2025-07-05 07:02:20 - model.monotonic_flow - INFO - Initial theta_raw: Parameter containing:
tensor([-2.9444, -2.7678, -2.6150, -2.4800, -2.3589, -2.2488, -2.1477, -2.0541,
        -1.9669, -1.8850, -1.8078, -1.7346, -1.6650, -1.5986, -1.5349, -1.4738,
        -1.4149, -1.3581, -1.3031, -1.2498, -1.1981, -1.1477, -1.0986, -1.0507,
        -1.0039, -0.9580, -0.9131, -0.8690, -0.8257, -0.7832, -0.7413, -0.7000,
        -0.6592, -0.6190, -0.5793, -0.5400, -0.5011, -0.4626, -0.4244, -0.3866,
        -0.3490, -0.3116, -0.2744, -0.2375, -0.2007, -0.1640, -0.1274, -0.0910,
        -0.0546, -0.0182,  0.0182,  0.0546,  0.0910,  0.1274,  0.1640,  0.2007,
         0.2375,  0.2744,  0.3116,  0.3490,  0.3866,  0.4244,  0.4626,  0.5011,
         0.5400,  0.5793,  0.6190,  0.6592,  0.7000,  0.7413,  0.7832,  0.8257,
         0.8690,  0.9131,  0.9580,  1.0039,  1.0507,  1.0986,  1.1477,  1.1981,
         1.2498,  1.3031,  1.3581,  1.4149,  1.4738,  1.5349,  1.5986,  1.6650,
         1.7346,  1.8078,  1.8850,  1.9669,  2.0541,  2.1477,  2.2488,  2.3589,
         2.4800,  2.6150,  2.7678,  2.9444], requires_grad=True)
2025-07-05 07:02:20 - model.monotonic_flow - INFO - Initialized MonotonicBasis with 100 basis functions
2025-07-05 07:02:20 - model.monotonic_flow - INFO - Initialized MonotonicFlowPredictor (per_layer=False, num_layers=N/A, SNR range=[3.0, 15.0], flow range=[0.0, 1.0])
2025-07-05 07:02:20 - model.transformer - INFO - Initialized monotonic flow predictor (per_layer=False, num_basis=100, flow_range=[0.0, 1.0])
2025-07-05 07:02:20 - model.transformer - INFO - GMMTransformer created with 197,192 parameters
2025-07-05 07:02:20 - model.factory - INFO - Created cluster prediction model with 197,704 parameters
2025-07-05 07:03:25 - config.base - INFO - Configuration loaded from output/final_experiments/baseline_64_layers/config.json
2025-07-05 07:03:25 - model.transformer - INFO - Using flow distribution mode: direct
2025-07-05 07:03:25 - model.transformer - INFO - Using standard attention with flash attention: True
2025-07-05 07:03:25 - model.transformer - INFO - Using layerwise repetition mode with 64 effective layers
2025-07-05 07:03:25 - model.transformer - INFO - Unique layers: 1, repeat factor: 64
2025-07-05 07:03:25 - model.transformer - INFO - Flow distribution mode: direct
2025-07-05 07:03:25 - model.monotonic_flow - INFO - Num basis: 100
2025-07-05 07:03:25 - model.monotonic_flow - INFO - Initial thetas: tensor([0.0050, 0.0150, 0.0250, 0.0350, 0.0450, 0.0550, 0.0650, 0.0750, 0.0850,
        0.0950, 0.1050, 0.1150, 0.1250, 0.1350, 0.1450, 0.1550, 0.1650, 0.1750,
        0.1850, 0.1950, 0.2050, 0.2150, 0.2250, 0.2350, 0.2450, 0.2550, 0.2650,
        0.2750, 0.2850, 0.2950, 0.3050, 0.3150, 0.3250, 0.3350, 0.3450, 0.3550,
        0.3650, 0.3750, 0.3850, 0.3950, 0.4050, 0.4150, 0.4250, 0.4350, 0.4450,
        0.4550, 0.4650, 0.4750, 0.4850, 0.4950, 0.5050, 0.5150, 0.5250, 0.5350,
        0.5450, 0.5550, 0.5650, 0.5750, 0.5850, 0.5950, 0.6050, 0.6150, 0.6250,
        0.6350, 0.6450, 0.6550, 0.6650, 0.6750, 0.6850, 0.6950, 0.7050, 0.7150,
        0.7250, 0.7350, 0.7450, 0.7550, 0.7650, 0.7750, 0.7850, 0.7950, 0.8050,
        0.8150, 0.8250, 0.8350, 0.8450, 0.8550, 0.8650, 0.8750, 0.8850, 0.8950,
        0.9050, 0.9150, 0.9250, 0.9350, 0.9450, 0.9550, 0.9650, 0.9750, 0.9850,
        0.9950])
2025-07-05 07:03:25 - model.monotonic_flow - INFO - Initial theta_raw: Parameter containing:
tensor([-2.9444, -2.7678, -2.6150, -2.4800, -2.3589, -2.2488, -2.1477, -2.0541,
        -1.9669, -1.8850, -1.8078, -1.7346, -1.6650, -1.5986, -1.5349, -1.4738,
        -1.4149, -1.3581, -1.3031, -1.2498, -1.1981, -1.1477, -1.0986, -1.0507,
        -1.0039, -0.9580, -0.9131, -0.8690, -0.8257, -0.7832, -0.7413, -0.7000,
        -0.6592, -0.6190, -0.5793, -0.5400, -0.5011, -0.4626, -0.4244, -0.3866,
        -0.3490, -0.3116, -0.2744, -0.2375, -0.2007, -0.1640, -0.1274, -0.0910,
        -0.0546, -0.0182,  0.0182,  0.0546,  0.0910,  0.1274,  0.1640,  0.2007,
         0.2375,  0.2744,  0.3116,  0.3490,  0.3866,  0.4244,  0.4626,  0.5011,
         0.5400,  0.5793,  0.6190,  0.6592,  0.7000,  0.7413,  0.7832,  0.8257,
         0.8690,  0.9131,  0.9580,  1.0039,  1.0507,  1.0986,  1.1477,  1.1981,
         1.2498,  1.3031,  1.3581,  1.4149,  1.4738,  1.5349,  1.5986,  1.6650,
         1.7346,  1.8078,  1.8850,  1.9669,  2.0541,  2.1477,  2.2488,  2.3589,
         2.4800,  2.6150,  2.7678,  2.9444], requires_grad=True)
2025-07-05 07:03:25 - model.monotonic_flow - INFO - Initialized MonotonicBasis with 100 basis functions
2025-07-05 07:03:25 - model.monotonic_flow - INFO - Initialized MonotonicFlowPredictor (per_layer=False, num_layers=N/A, SNR range=[3.0, 15.0], flow range=[0.0, 1.0])
2025-07-05 07:03:25 - model.transformer - INFO - Initialized monotonic flow predictor (per_layer=False, num_basis=100, flow_range=[0.0, 1.0])
2025-07-05 07:03:25 - model.transformer - INFO - GMMTransformer created with 197,192 parameters
2025-07-05 07:03:25 - model.factory - INFO - Created cluster prediction model with 197,704 parameters
2025-07-05 07:05:04 - config.base - INFO - Configuration loaded from output/final_experiments/no_flow_16_layers/config.json
2025-07-05 07:05:04 - model.transformer - INFO - Using flow distribution mode: direct
2025-07-05 07:05:04 - model.transformer - INFO - Using standard attention with flash attention: True
2025-07-05 07:05:04 - model.transformer - INFO - Using layerwise repetition mode with 16 effective layers
2025-07-05 07:05:04 - model.transformer - INFO - Unique layers: 1, repeat factor: 16
2025-07-05 07:05:04 - model.transformer - INFO - Flow distribution mode: direct
2025-07-05 07:05:04 - model.transformer - INFO - GMMTransformer created with 196,992 parameters
2025-07-05 07:05:04 - model.factory - INFO - Created cluster prediction model with 197,504 parameters
